# ================================================================
# GIDEON AI ASSISTANT - PRODUCTION REQUIREMENTS
# 100% LOCAL avec Ollama - AUCUNE dépendance externe
# Date: 2024 - Migration complète vers Ollama local
# ================================================================

# =====================================
# CORE DEPENDENCIES - CRITICAL
# =====================================

# Interface graphique - STABLE (évite bugs 6.7+)
PyQt6>=6.4.0,<6.7.0

# Computing base
numpy>=1.21.0,<1.26.0          # Compatible avec tout
requests>=2.28.0,<3.0.0        # HTTP requests pour Ollama
psutil>=5.9.0,<6.0.0           # System monitoring

# =====================================
# AUDIO STACK - STABLE (résout Problème #2)
# =====================================

# Remplacement pyaudio - PLUS STABLE
sounddevice>=0.4.6,<0.5.0      # Audio I/O multiplateforme
soundfile>=0.12.1,<0.13.0      # Audio file handling
scipy>=1.9.0,<1.12.0           # Signal processing

# Text-to-Speech - COMPATIBLE
pyttsx3>=2.90,<3.0.0           # TTS multiplateforme
SpeechRecognition>=3.10.0,<4.0.0  # Voice recognition

# =====================================
# COMPUTER VISION - LIGHTWEIGHT (résout Problème #3)
# =====================================

# OpenCV - VERSION STABLE
opencv-python>=4.7.0,<4.9.0    # Computer vision

# Face Detection - ALTERNATIVE SANS COMPILATION
mtcnn>=0.1.1,<1.0.0           # Face detection lightweight
tensorflow-cpu>=2.10.0,<2.14.0  # Backend MTCNN (CPU only)
Pillow>=9.0.0,<11.0.0         # Image processing

# =====================================
# LOCAL AI avec Ollama - 100% OFFLINE
# =====================================

# Pas de dépendance OpenAI - Ollama utilise HTTP requests
# requests déjà inclus ci-dessus

# Embeddings et mémoire vectorielle locale
sentence-transformers>=2.2.0   # Local embeddings
chromadb>=0.4.0                # Local vector database
transformers>=4.30.0           # Hugging Face transformers

# =====================================
# DEVELOPMENT & QUALITY
# =====================================

# Formatage et linting
black>=23.0.0,<24.0.0
flake8>=6.0.0,<7.0.0
isort>=5.12.0,<6.0.0

# Testing
pytest>=7.0.0,<8.0.0
pytest-cov>=4.0.0,<5.0.0

# =====================================
# OS-SPECIFIC FALLBACKS
# =====================================

# Windows specific
pywin32>=306; sys_platform == "win32"

# macOS specific (permissions handling)
pyobjc-framework-Cocoa>=9.0; sys_platform == "darwin"
pyobjc-framework-AVFoundation>=9.0; sys_platform == "darwin"

# =====================================
# OPTIONAL DEPENDENCIES
# =====================================

# Smart home (Philips Hue)
# requests déjà inclus ci-dessus

# Performance monitoring
memory-profiler>=0.60.0        # Memory usage tracking

# =====================================
# VERSION LOCKS CRITIQUES
# Évite les conflits de dépendances (Problème #4)
# =====================================

# Numpy version compatible avec tout
# TensorFlow et OpenCV ont des exigences strictes
# PyQt6 nécessite numpy >= 1.21
# TensorFlow-cpu nécessite numpy < 1.26

# MATRIX DE COMPATIBILITÉ TESTÉE:
# Python 3.9 + numpy 1.23 + tensorflow 2.12 + opencv 4.8 = ✅
# Python 3.10 + numpy 1.24 + tensorflow 2.12 + opencv 4.8 = ✅ 
# Python 3.11 + numpy 1.25 + tensorflow 2.13 + opencv 4.8 = ✅

# =====================================
# INSTALLATION NOTES - OLLAMA
# =====================================

# Pour migration OpenAI -> Ollama:
# 1. Désinstaller OpenAI: pip uninstall openai
# 2. Installer Ollama: brew install ollama (macOS) ou voir ollama.ai
# 3. Démarrer Ollama: ollama serve
# 4. Installer modèles: ollama pull mistral:7b && ollama pull llama3:8b
# 5. Tester: curl http://localhost:11434/api/tags

# Alternative si échec:
# conda create -n gideon python=3.9
# conda activate gideon
# pip install -r requirements_production.txt 
# ollama serve 